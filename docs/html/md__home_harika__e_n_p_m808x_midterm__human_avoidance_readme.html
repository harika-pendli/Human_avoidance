<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Human Detection and Tracker: Human Detection and Tracking</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Human Detection and Tracker
   &#160;<span id="projectnumber">0.02</span>
   </div>
   <div id="projectbrief">Acme INC. &#39;s perception module for human detection and tracking humans in robot&#39;s reference frame</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md__home_harika__e_n_p_m808x_midterm__human_avoidance_readme.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Human Detection and Tracking </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a href="https://opensource.org/licenses/MIT"><object type="image/svg+xml" data="https://img.shields.io/badge/License-MIT-green.svg" style="pointer-events: none;">License: MIT</object></a> <a href="https://github.com/shaileshpranav/Human_avoidance/actions/workflows/CI.yml"><object type="image/svg+xml" data="https://github.com/shaileshpranav/Human_avoidance/actions/workflows/CI.yml/badge.svg" style="pointer-events: none;">C/C++ CI</object></a> <a href="https://coveralls.io/github/shaileshpranav/Human_avoidance?branch=main"><object type="image/svg+xml" data="https://coveralls.io/repos/github/shaileshpranav/Human_avoidance/badge.svg?branch=main" style="pointer-events: none;">Coverage Status</object></a></p>
<h1><a class="anchor" id="autotoc_md1"></a>
Overview</h1>
<p>Human detection and tracking project for Midterm of course ENPM 808X</p>
<p>Acme Robotics Inc. is a private company set to launch a 4-wheeled robot used to deliver packages inside office for an undisclosed multinational company next. This robot is set to debut early next year. It moves in the corridor at a walking pace. The package is stored inside of the robot and drives itself to the customer. They have given us complete ownership of designing and developing a new feature for this robot's perceeption stack -"Human detector and tracker".</p>
<p>Object detection is a very important computer vision task. Human detection is the task of locating all instances of human beings present in an image, and it has been most widely accomplished by searching all locations in the image, at all possible scales, and comparing a small area at each location with known templates or patterns of people. Human tracking is the process of temporally associating the human detections within a video sequence to generate persistent paths, or trajectories, of the people. Human detection and tracking are generally considered the first two processes in a video surveillance pipeline, and can feed into higher-level reasoning modules such as action recognition and dynamic scene analysis. Object detection and tracking is of utmost importance for different kinds of applications such as safety, surveillance, man-machine interaction, driving assistance system, traffic monitoring. Finding people in images has attracted much attention in recent years for practical applications such as visual surveillance. The detection of a human being is important for abnormal event detection, human gait characterization, people counting, person identification and tracking, pedestrian detection, gender classification. Human detection and tracking are tasks of computer vision systems for locating and following people in video imagery.</p>
<p>This is an example of human detection.</p>
<p><img src="docs/detect.jpeg" alt="Detect_Ex" title="Detect_Ex" class="inline"/></p>
<p>And This is an example of human tracking.</p>
<p><img src="docs/track.jpeg" alt="Track_Ex" title="Track_Ex" class="inline"/></p>
<p>In this module, we aim to build a module which when recieves video feed, starts detecting humans in the frame and gives ID to individual instances and tracks them over rest of the frames. These coordinates are then transformed into the robots reference frame and the final output is their (x,y,z) coordinate with respect to the robot reference frame.</p>
<h1><a class="anchor" id="autotoc_md2"></a>
Authors</h1>
<ul>
<li><a href="https://github.com/shaileshpranav">Shailesh Pranav Rajendran</a></li>
<li><a href="https://github.com/harika-pendli">Harika Pendli</a></li>
</ul>
<h1><a class="anchor" id="autotoc_md3"></a>
License</h1>
<div class="fragment"><div class="line">MIT License</div>
<div class="line"> </div>
<div class="line">Copyright (c) 2022 Shailesh Pranav Rajendran, Harika Pendli</div>
<div class="line"> </div>
<div class="line">Permission is hereby granted, free of charge, to any person obtaining a copy</div>
<div class="line">of this software and associated documentation files (the &quot;Software&quot;), to deal</div>
<div class="line">in the Software without restriction, including without limitation the rights</div>
<div class="line">to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</div>
<div class="line">copies of the Software, and to permit persons to whom the Software is</div>
<div class="line">furnished to do so, subject to the following conditions:</div>
<div class="line"> </div>
<div class="line">The above copyright notice and this permission notice shall be included in all</div>
<div class="line">copies or substantial portions of the Software.</div>
<div class="line"> </div>
<div class="line">THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</div>
<div class="line">IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</div>
<div class="line">FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</div>
<div class="line">AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</div>
<div class="line">LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</div>
<div class="line">OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</div>
<div class="line">SOFTWARE.</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md4"></a>
Links</h1>
<ul>
<li><a href="https://github.com/shaileshpranav/Human_avoidance/blob/main/Proposal.pdf">Phase-0 Proposal</a></li>
<li><a href="https://drive.google.com/file/d/1JcN-jdWsAfGG5XlVniN_F_-hbOWnSuIF/view?usp=sharing">Phase-0 Proposal Video</a></li>
<li><a href="https://docs.google.com/spreadsheets/d/13NfVb0g8LwvVlH9F1EcA1EnQb8gwcYmieM8Q2csQmX0/edit#gid=0">Product-log</a></li>
<li><a href="https://docs.google.com/document/d/1Nb_psVTufyzcFsdD67MJTqQvALrAlez0QEYVcEus4a4/edit">Sprint Review</a></li>
<li><a href="https://drive.google.com/file/d/1zdG5oJxihnf690Nj9lmH-NxazA3RiWn2/view?usp=sharing">Phase 1 Update Video</a></li>
</ul>
<h1><a class="anchor" id="autotoc_md5"></a>
Demo</h1>
<p><img src="output/Result.gif" alt="" class="inline"/></p>
<h1><a class="anchor" id="autotoc_md6"></a>
Design and Development process</h1>
<p>We made use of ESC methodology for the initial design process which is by extraction of significant concepts (ESC). We have identified the classes of the future program and their responsibilities. The class relations have been explained through UML class diagrams which cn be found in the <a href="https://github.com/harika-pendli/Human_avoidance/blob/dev2/uml/revised/UML_final.png">UML revised folder</a></p>
<p>Our software team worked on this project through iterative software evolution and service processes. We engaged in Agile Iterative Development Process (AIP) through Test-Driven Development during the entire project period. Being a team of two programmers, we have decided to adopt pair programming and switch roles as navigator and drive as and when necessary.</p>
<h1><a class="anchor" id="autotoc_md7"></a>
Algorithm and methodology</h1>
<p>The perception pipeline flows as follows: command parser in data loader classes parses the arguments to retrieve the input to the pipeline (image/video), which then goes to the Human detector class which acts like a driver class. Here the yolo model config class and the transformation class intervenes to detect humans and get the depth or the distance of the humans from the camera respectively. We also have a Tracking stub class which we aim to update and release this functionality in the next version 0.02.</p>
<p>First, the class Human detector is initialized, which in turn initializes, the other two classes namely, the modelConfig class, and the data loader class. Here detection takes place for each frame (if provided with video input) or an image based on the pre-trained <a href="https://pjreddie.com/darknet/yolo/">YOLOv3 model</a>. This detection is then passed to the transformation class where the x,y,z coordinate in the robot frame is calculated and the y-coordinate (depth) or the distance of the person from the robot is calculated (transformation from camera frame to robot reference frame) and shown on the image or video output. Currently we have made a basic tracker system but we plan to release it in the next build.</p>
<p>For the unimplemented tracker class, we plan to do the following: The updated coordinates along with the frame will be sent to the tracker function of the <a class="el" href="class_tracker.html" title="class that tracks the objects from all frames of the video feed">Tracker</a> class where detection and tracking will be managed by the <a href="https://github.com/nwojke/deep_sort">DeepSORT</a> deeplearning model. Here the detected objects are assigned unique ids. This id along with the frame id as well as the coordinate of the object in the camera frame is saved in an array. This process is repeated until all the image frames are completed.</p>
<h1><a class="anchor" id="autotoc_md8"></a>
Known Issues/Risks</h1>
<ul>
<li>Missed detection: It may happen that the model sometimes misses some human objects during detection. In such a case, deploying more than one and different models can be viable.</li>
<li>Duplicate detections: It can happen due to low lighting or bad lighting conditions or bas resolution and quality of the video or the image.</li>
</ul>
<h1><a class="anchor" id="autotoc_md9"></a>
Install Dependencies</h1>
<ul>
<li>Ubuntu 20.04(LTS)</li>
<li>CMake</li>
<li>OpenCV</li>
<li>Github CI</li>
<li>Coveralls</li>
<li>Git <div class="fragment"><div class="line">sh dependencies.sh</div>
</div><!-- fragment --></li>
</ul>
<h1><a class="anchor" id="autotoc_md10"></a>
Build via command-line</h1>
<div class="fragment"><div class="line">git clone --recursive https://github.com/shaileshpranav/Human_avoidance</div>
<div class="line">cd &lt;path to repository&gt;</div>
<div class="line">sh script.sh</div>
<div class="line">mkdir build &amp;&amp; cd build</div>
<div class="line">cmake ..</div>
<div class="line">make</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md11"></a>
Run Test:</h1>
<pre class="fragment">./test/cpp-test
</pre> <h1><a class="anchor" id="autotoc_md12"></a>
Run program:</h1>
<ul>
<li>For Image <div class="fragment"><div class="line">./app/app --image=../input/1.png</div>
</div><!-- fragment --></li>
<li>For video <div class="fragment"><div class="line">./app/app --video=../input/video.mp4</div>
</div><!-- fragment --></li>
</ul>
<h1><a class="anchor" id="autotoc_md13"></a>
Building for code coverage</h1>
<div class="fragment"><div class="line">sudo apt-get install lcov</div>
<div class="line">cmake -D COVERAGE=ON -D CMAKE_BUILD_TYPE=Debug ../</div>
<div class="line">make</div>
<div class="line">make code_coverage</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md14"></a>
Plugins</h1>
<ul>
<li>Google C++ Sytle <div class="fragment"><div class="line">clang-format -style=Google -i your_file.cpp</div>
</div><!-- fragment --></li>
<li>Cpplint</li>
</ul>
<div class="fragment"><div class="line"># You may need to install cpplint:</div>
<div class="line">sudo apt install python3-pip</div>
<div class="line">pip install cpplint</div>
<div class="line"> </div>
<div class="line"># read the cpplint manual to get an idea of what it does:</div>
<div class="line">~/.local/bin/cpplint -h</div>
<div class="line">#to run on a file:</div>
<div class="line">cpplint &quot;FIlename.cpp&quot;</div>
</div><!-- fragment --><ul>
<li>cppCheck <div class="fragment"><div class="line">cppcheck --enable=all --std=c++11 -I include/ --suppress=missingIncludeSystem $( find . -name *.cpp | grep -vE -e &quot;^./build/&quot; -e &quot;^./vendor/&quot; )</div>
</div><!-- fragment --></li>
</ul>
<h1><a class="anchor" id="autotoc_md15"></a>
Running Doxygen</h1>
<div class="fragment"><div class="line">sudo apt-install doxywizard</div>
<div class="line">run doxywizard</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
